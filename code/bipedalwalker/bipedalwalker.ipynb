{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar imports\n",
    "import sys, os, time, importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Gym imports\n",
    "import gym\n",
    "from gym.vector import SyncVectorEnv\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "# Custom imports\n",
    "sys.path.append(os.path.abspath('..')) # Add parent directory to path\n",
    "\n",
    "import ppo_network\n",
    "importlib.reload(ppo_network) # Prevents caching issues with notebooks\n",
    "from ppo_network import PPONetworkContinuous\n",
    "\n",
    "import ppo\n",
    "importlib.reload(ppo) # Prevents caching issues with notebooks\n",
    "from ppo import PPOContinuous\n",
    "\n",
    "import hp_tuner\n",
    "importlib.reload(hp_tuner) # Prevents caching issues with notebooks\n",
    "from hp_tuner import HPTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BipedalWalker environment\n",
    "env_id = 'BipedalWalker-v3'\n",
    "max_episode_steps = 1024\n",
    "num_envs = 16\n",
    "\n",
    "env_kwargs = {\n",
    "    'id': env_id,\n",
    "    'max_episode_steps': max_episode_steps,\n",
    "}\n",
    "\n",
    "# Create vectorized environment\n",
    "envs_vector = SyncVectorEnv([lambda: gym.make(**env_kwargs)] * num_envs)\n",
    "states, infos = envs_vector.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean-var-value network\n",
    "network_kwargs = {\n",
    "    'input_dims': 24,\n",
    "    'output_dims': 4,\n",
    "    'shared_hidden_dims': [1024, 1024, 512],\n",
    "    'shared_norm': nn.LayerNorm,\n",
    "    'shared_activation': nn.ReLU,\n",
    "    'mean_hidden_dims': [512, 256, 128, 64],\n",
    "    'mean_norm': nn.LayerNorm,\n",
    "    'mean_activation': nn.ReLU,\n",
    "    'log_var_hidden_dims': [512, 256, 128, 64],\n",
    "    'log_var_norm': nn.LayerNorm,\n",
    "    'log_var_activation': nn.ReLU,\n",
    "    'value_hidden_dims': [512, 256, 128, 64],\n",
    "    'value_norm': nn.LayerNorm,\n",
    "    'value_activation': nn.ReLU,\n",
    "}\n",
    "\n",
    "# Create the mean-var-value network\n",
    "network = PPONetworkContinuous(**network_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/axelsolhall/miniconda3/envs/PPOgym/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/Users/axelsolhall/miniconda3/envs/PPOgym/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:253: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'torch.Tensor'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [-0.02082795 -0.03081958 -0.03166328 -0.0137893   0.477930\n",
      "State: [-0.03141257 -0.02118443 -0.01232278 -0.01719513  0.522367\n",
      "State: [-0.05009324 -0.03701037 -0.01954351 -0.02008068  0.589879\n",
      "State: [-0.06876309 -0.03750424 -0.01577852 -0.02837856  0.660927\n",
      "State: [-0.09932724 -0.06123087 -0.0172622  -0.00654481  0.717406\n",
      "State: [-0.12633781 -0.05424867 -0.01464321 -0.03339153  0.80065 \n",
      "State: [-0.14317894 -0.03389863 -0.012965   -0.06695844  0.879454\n",
      "State: [-0.16906707 -0.05184544 -0.01146565 -0.07258829  0.958023\n",
      "State: [-1.8185523e-01 -2.5651516e-02 -9.9529477e-04 -9.0283722e-\n",
      "State: [-0.20715906 -0.0506266  -0.0123196  -0.11230776  1.066414\n"
     ]
    }
   ],
   "source": [
    "# Test forward passes\n",
    "for _ in range(10):\n",
    "    states_tensor = torch.tensor(states, dtype=torch.float32)\n",
    "    mean, log_var, value = network(states_tensor)\n",
    "    std_dev = torch.exp(log_var / 2)\n",
    "    \n",
    "    actions_dist = torch.distributions.Normal(mean, std_dev)\n",
    "    actions = actions_dist.sample()\n",
    "    \n",
    "    states, rewards, dones, truncateds, infos = envs_vector.step(actions)\n",
    "    print(f\"State: {states[0]}\"[:65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-100.48])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PPO hyperparameters\n",
    "ppo_kwargs = {\n",
    "    'network_class': PPONetworkContinuous,\n",
    "    'network_kwargs': network_kwargs,\n",
    "    'action_dims': 4,\n",
    "    'num_envs': num_envs,\n",
    "    'lr': 3e-4,\n",
    "    'final_lr': 5e-6,\n",
    "    'gamma': 0.99,\n",
    "    'lam': 0.95,\n",
    "    'clip_eps': 0.25,\n",
    "    'final_clip_eps': 0.025,\n",
    "    'value_coef': 0.7,\n",
    "    'entropy_coef': 0.05,\n",
    "    'final_entropy_coef': 0.025,\n",
    "    'batch_size': 2048,\n",
    "    'mini_batch_size': 512,\n",
    "    'batch_epochs': 8,\n",
    "    'batch_shuffle': True,\n",
    "    'seperate_envs_shuffle': True,\n",
    "    'reward_normalize': True,\n",
    "    'truncated_reward': 50,\n",
    "    'debug_prints': False,\n",
    "}\n",
    "\n",
    "ppo = PPOContinuous(envs_vector, **ppo_kwargs)\n",
    "\n",
    "# Test training\n",
    "ppo.train(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization\n",
    "hp_tuner = HPTuner(\n",
    "    env_kwargs=env_kwargs,\n",
    "    num_envs=num_envs,\n",
    "    ppo_class=PPOContinuous,\n",
    "    ppo_kwargs=ppo_kwargs,\n",
    ")\n",
    "\n",
    "# Define hyperparameters to optimize\n",
    "parameters = [\n",
    "    ('entropy_coef', [0.1, -0.1]),\n",
    "    ('batch_size', [64, 128, 256, 512]),\n",
    "    ('batch_epochs', [2, 4, 8, 16]),\n",
    "    ]\n",
    "\n",
    "# Optimize hyperparameters\n",
    "evolutions = hp_tuner.optimize_hyperparameters(\n",
    "    parameters, generations=50, num_trials = 16,\n",
    "    )\n",
    "\n",
    "# Save evolution data\n",
    "hp_tuner.evolution_video(\n",
    "    generations=100, video_folder = 'videos', increments=10, max_frames=max_episode_steps,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPOgym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
