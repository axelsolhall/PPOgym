{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Enable autoreload for development\n",
    "import IPython\n",
    "ipython = IPython.get_ipython()\n",
    "ipython.run_line_magic('load_ext', 'autoreload')\n",
    "ipython.run_line_magic('autoreload', '2')\n",
    "\n",
    "# Auxiliar imports\n",
    "import sys, os, time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Gym imports\n",
    "import gym\n",
    "from gym.vector import SyncVectorEnv\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "# Custom imports\n",
    "sys.path.append(os.path.abspath('..')) # Add parent directory to path\n",
    "\n",
    "from ppo_network import PPONetworkContinuous\n",
    "from ppo import PPOContinuous\n",
    "from hp_tuner import HPTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BipedalWalker environment\n",
    "env_id = 'BipedalWalker-v3'\n",
    "max_episode_steps = 1024\n",
    "num_envs = 16\n",
    "\n",
    "env_kwargs = {\n",
    "    'id': env_id,\n",
    "    'max_episode_steps': max_episode_steps,\n",
    "}\n",
    "\n",
    "# Create vectorized environment\n",
    "envs_vector = SyncVectorEnv([lambda: gym.make(**env_kwargs)] * num_envs)\n",
    "states, infos = envs_vector.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean-var-value network\n",
    "network_kwargs = {\n",
    "    'input_dims': 24,\n",
    "    'output_dims': 4,\n",
    "    'shared_hidden_dims': [1024, 1024, 512],\n",
    "    'shared_norm': nn.LayerNorm,\n",
    "    'shared_activation': nn.SiLU,\n",
    "    'mean_hidden_dims': [512, 256, 128, 64],\n",
    "    'mean_norm': nn.LayerNorm,\n",
    "    'mean_activation': nn.SiLU,\n",
    "    'log_var_hidden_dims': [512, 256, 128, 64],\n",
    "    'log_var_norm': nn.LayerNorm,\n",
    "    'log_var_activation': nn.SiLU,\n",
    "    'value_hidden_dims': [512, 256, 128, 64],\n",
    "    'value_norm': nn.LayerNorm,\n",
    "    'value_activation': nn.SiLU,\n",
    "}\n",
    "\n",
    "# Create the mean-var-value network\n",
    "network = PPONetworkContinuous(**network_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [-0.02630167 -0.04093861 -0.0264974  -0.00217259  0.471512\n",
      "State: [-0.05390408 -0.05521606 -0.02996548 -0.01982511  0.511444\n",
      "State: [-0.06219898 -0.01661935 -0.00316062  0.01299398  0.532144\n",
      "State: [-0.09121178 -0.05808256 -0.02399905  0.00456464  0.611245\n",
      "State: [-0.11475505 -0.0471875  -0.01811942 -0.01543638  0.689527\n",
      "State: [-0.15634106 -0.08339607 -0.03936466 -0.0361727   0.791444\n",
      "State: [-0.21213384 -0.11151683 -0.04510277 -0.0315982   0.900696\n",
      "State: [-0.25759536 -0.09099551 -0.03491103 -0.05181476  0.978399\n",
      "State: [-0.3035023  -0.09192593 -0.0350437  -0.0696331   1.056652\n",
      "State: [-0.357984   -0.1091228  -0.03873978 -0.07200801  1.135344\n"
     ]
    }
   ],
   "source": [
    "# Test forward passes\n",
    "for _ in range(3):\n",
    "    states_tensor = torch.tensor(states, dtype=torch.float32)\n",
    "    mean, log_var, value = network(states_tensor)\n",
    "    std_dev = torch.exp(log_var / 2)\n",
    "    \n",
    "    actions_dist = torch.distributions.Normal(mean, std_dev)\n",
    "    actions = actions_dist.sample().detach().numpy()\n",
    "    \n",
    "    states, rewards, dones, truncateds, infos = envs_vector.step(actions)\n",
    "    print(f\"State: {states[0]}\"[:65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-124.14])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PPO hyperparameters\n",
    "ppo_kwargs = {\n",
    "    'network_class': PPONetworkContinuous,\n",
    "    'network_kwargs': network_kwargs,\n",
    "    'action_dims': 4,\n",
    "    'num_envs': num_envs,\n",
    "    'lr': 3e-4,\n",
    "    'final_lr': 5e-6,\n",
    "    'gamma': 0.99,\n",
    "    'lam': 0.95,\n",
    "    'clip_eps': 0.25,\n",
    "    'final_clip_eps': 0.025,\n",
    "    'value_coef': 0.7,\n",
    "    'entropy_coef': 0.05,\n",
    "    'final_entropy_coef': 0.025,\n",
    "    'batch_size': 2048,\n",
    "    'mini_batch_size': 512,\n",
    "    'batch_epochs': 8,\n",
    "    'batch_shuffle': True,\n",
    "    'seperate_envs_shuffle': True,\n",
    "    'reward_normalize': True,\n",
    "    'truncated_reward': 0,\n",
    "    'debug_prints': False,\n",
    "}\n",
    "\n",
    "ppo = PPOContinuous(envs_vector, **ppo_kwargs)\n",
    "\n",
    "# Test training\n",
    "ppo.train(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing entropy_coef with values: [0.1, -0.1]\n",
      "Running trials for entropy_coef = 0.1\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter optimization\n",
    "hp_tuner = HPTuner(\n",
    "    env_kwargs=env_kwargs,\n",
    "    num_envs=num_envs,\n",
    "    ppo_class=PPOContinuous,\n",
    "    ppo_kwargs=ppo_kwargs,\n",
    ")\n",
    "\n",
    "# Define hyperparameters to optimize\n",
    "parameters = [\n",
    "    ('entropy_coef', [0.1, -0.1]),\n",
    "    ('batch_size', [64, 128, 256, 512]),\n",
    "    ('batch_epochs', [2, 4, 8, 16]),\n",
    "    ]\n",
    "\n",
    "# Optimize hyperparameters\n",
    "evolutions = hp_tuner.optimize_hyperparameters(\n",
    "    parameters, generations=50, num_trials = 16,\n",
    "    )\n",
    "\n",
    "# Save evolution data\n",
    "hp_tuner.evolution_video(\n",
    "    generations=100, video_folder = 'videos', increments=20, max_frames=max_episode_steps,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPOgym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
