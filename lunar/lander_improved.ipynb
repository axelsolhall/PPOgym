{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar imports\n",
    "import sys, os, time, importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Gym imports\n",
    "import gym\n",
    "from gym.vector import SyncVectorEnv\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "# Custom imports\n",
    "sys.path.append(os.path.abspath('..')) # Add parent directory to path\n",
    "\n",
    "import ppo_network\n",
    "importlib.reload(ppo_network) # Prevents caching issues with notebooks\n",
    "from ppo_network import PPONetworkDiscrete\n",
    "\n",
    "import ppo\n",
    "importlib.reload(ppo) # Prevents caching issues with notebooks\n",
    "from ppo import PPODiscrete\n",
    "\n",
    "# import ppo_restored\n",
    "# importlib.reload(ppo_restored) # Prevents caching issues with notebooks\n",
    "# from ppo_restored import PPOWrapper\n",
    "\n",
    "import hp_optimizer\n",
    "importlib.reload(hp_optimizer) # Prevents caching issues with notebooks\n",
    "from hp_optimizer import HPOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LunarLander environment\n",
    "env_id = 'LunarLander-v2'\n",
    "max_episode_steps = 1024\n",
    "num_envs = 16\n",
    "\n",
    "env_kwargs = {\n",
    "    'id': env_id,\n",
    "    'max_episode_steps': max_episode_steps,\n",
    "}\n",
    "\n",
    "# Create vectorized environment\n",
    "envs_vector = SyncVectorEnv([lambda: gym.make(**env_kwargs)] * num_envs)\n",
    "states, infos = envs_vector.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy-Value Network\n",
    "# TODO - Move to PPO-kwargs\n",
    "input_dims = 8\n",
    "output_dims = 4\n",
    "\n",
    "shared_hidden_dims = [1024, 512, 256]\n",
    "shared_norm = nn.LayerNorm\n",
    "shared_activation = nn.SiLU\n",
    "\n",
    "policy_hidden_dims = [256, 128, 64]\n",
    "policy_norm = nn.LayerNorm\n",
    "policy_activation = nn.SiLU\n",
    "\n",
    "value_hidden_dims = [256, 128, 64]\n",
    "value_norm = nn.LayerNorm\n",
    "value_activation = nn.SiLU\n",
    "\n",
    "network_kwargs = {\n",
    "    'input_dims': input_dims,\n",
    "    'output_dims': output_dims,\n",
    "    \n",
    "    'shared_hidden_dims': shared_hidden_dims,\n",
    "    'shared_norm': shared_norm,\n",
    "    'shared_activation': shared_activation,\n",
    "    \n",
    "    'policy_hidden_dims': policy_hidden_dims,\n",
    "    'policy_norm': policy_norm,\n",
    "    'policy_activation': policy_activation,\n",
    "    \n",
    "    'value_hidden_dims': value_hidden_dims,\n",
    "    'value_norm': value_norm,\n",
    "    'value_activation': value_activation,\n",
    "}\n",
    "\n",
    "network = PPONetworkDiscrete(**network_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: per vectorized env: 0.00 s\n"
     ]
    }
   ],
   "source": [
    "# Test forward passes\n",
    "now = time.time()\n",
    "for _ in range(10):\n",
    "    states_tensor = torch.tensor(states, dtype=torch.float32)\n",
    "    policy, value = network(states_tensor)\n",
    "    \n",
    "    actions_dist = torch.distributions.Categorical(logits=policy)\n",
    "    actions = actions_dist.sample().numpy()\n",
    "    \n",
    "    states, rewards, dones, truncateds, infos = envs_vector.step(actions)\n",
    "    #print(dones)\n",
    "\n",
    "print(\n",
    "    f'Elapsed time: per vectorized env: {(time.time() - now)/num_envs:.2f} s'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation    0 - Reward:  -115.22, w/o trunc.:  -115.22\n",
      "Generation    1 - Reward:   -83.12, w/o trunc.:   -83.12\n",
      "Generation    2 - Reward:  -378.59, w/o trunc.:  -134.84\n",
      "Generation    3 - Reward:  -103.57, w/o trunc.:   -84.82\n",
      "Generation    4 - Reward:  -392.56, w/o trunc.:  -111.31\n",
      "Generation    5 - Reward:  -315.33, w/o trunc.:  -127.83\n",
      "Generation    6 - Reward:  -325.22, w/o trunc.:  -212.72\n",
      "Generation    7 - Reward:   123.89, w/o trunc.:   123.89\n",
      "Generation    8 - Reward:   -44.60, w/o trunc.:   -44.60\n",
      "Generation    9 - Reward:   115.18, w/o trunc.:   115.18\n",
      "Generation   10 - Reward:    97.95, w/o trunc.:    97.95\n",
      "Generation   11 - Reward:   248.72, w/o trunc.:   248.72\n",
      "Generation   12 - Reward:    86.85, w/o trunc.:   180.60\n",
      "Generation   13 - Reward:   123.71, w/o trunc.:   198.71\n",
      "Generation   14 - Reward:   223.57, w/o trunc.:   223.57\n",
      "Generation   15 - Reward:   244.69, w/o trunc.:   244.69\n",
      "Generation   16 - Reward:   188.47, w/o trunc.:   207.22\n",
      "Generation   17 - Reward:   200.48, w/o trunc.:   219.23\n",
      "Generation   18 - Reward:   194.97, w/o trunc.:   194.97\n",
      "Generation   19 - Reward:    94.84, w/o trunc.:   151.09\n",
      "Generation   20 - Reward:    42.78, w/o trunc.:   117.78\n",
      "Generation   21 - Reward:   124.14, w/o trunc.:   161.64\n",
      "Generation   22 - Reward:   125.75, w/o trunc.:   125.75\n",
      "Generation   23 - Reward:    96.23, w/o trunc.:    96.23\n",
      "Generation   24 - Reward:   168.61, w/o trunc.:   168.61\n",
      "Generation   25 - Reward:   115.17, w/o trunc.:   133.92\n",
      "Generation   26 - Reward:   146.84, w/o trunc.:   146.84\n",
      "Generation   27 - Reward:   -36.65, w/o trunc.:   132.10\n",
      "Generation   28 - Reward:   -16.38, w/o trunc.:   114.87\n",
      "Generation   29 - Reward:    82.69, w/o trunc.:   157.69\n",
      "Generation   30 - Reward:   147.13, w/o trunc.:   147.13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m ppo \u001b[38;5;241m=\u001b[39m PPODiscrete(envs_vector, network, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mppo_kwargs)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# now = time.time()\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# after = time.time()\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# print(f'Elapsed time: {after - now:.2f} s')\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# MPS:          - 110s\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#? So slow if device is selected, default is sooo much faster\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/PPOgym/ppo.py:335\u001b[0m, in \u001b[0;36mPPOBase.train\u001b[0;34m(self, generations, save_folder, save_sequence)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# Iterate through generations\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(generations):\n\u001b[1;32m    332\u001b[0m \n\u001b[1;32m    333\u001b[0m     \u001b[38;5;66;03m# Collect trajectories\u001b[39;00m\n\u001b[1;32m    334\u001b[0m     states, actions, log_probs, rewards, dones, truncateds, values \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 335\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     )\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m# Compute advantages\u001b[39;00m\n\u001b[1;32m    339\u001b[0m     advantages, returns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_advantages(\n\u001b[1;32m    340\u001b[0m         rewards, dones, truncateds, values\n\u001b[1;32m    341\u001b[0m     )\n",
      "File \u001b[0;32m~/dev/PPOgym/ppo.py:155\u001b[0m, in \u001b[0;36mPPOBase.collect_trajectories\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m states_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(current_states, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Get actions, log_probs and values\u001b[39;00m\n\u001b[1;32m    154\u001b[0m current_actions, current_log_probs, current_values \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_pass_collect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Step through environments\u001b[39;00m\n\u001b[1;32m    159\u001b[0m next_states, current_rewards, current_dones, current_truncateds, infos \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs\u001b[38;5;241m.\u001b[39mstep(current_actions\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    161\u001b[0m )\n",
      "File \u001b[0;32m~/dev/PPOgym/ppo.py:451\u001b[0m, in \u001b[0;36mPPODiscrete.forward_pass_collect\u001b[0;34m(self, states_tensor)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_pass_collect\u001b[39m(\u001b[38;5;28mself\u001b[39m, states_tensor):\n\u001b[0;32m--> 451\u001b[0m     policies, values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m     action_dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(policies)\n\u001b[1;32m    454\u001b[0m     actions \u001b[38;5;241m=\u001b[39m action_dist\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[0;32m~/miniconda3/envs/PPOgym/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PPOgym/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/PPOgym/ppo_network.py:105\u001b[0m, in \u001b[0;36mPPONetworkDiscrete.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Pass through shared layers\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Policy head\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     policy_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_layers(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/PPOgym/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PPOgym/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/PPOgym/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/PPOgym/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PPOgym/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/PPOgym/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# PPO \n",
    "\n",
    "lr = 3e-4\n",
    "final_lr = 5e-6\n",
    "\n",
    "gamma = 0.995\n",
    "lam = 0.99\n",
    "\n",
    "clip_eps = 0.25\n",
    "final_clip_eps = 0.01\n",
    "\n",
    "value_coef = 0.7\n",
    "\n",
    "entropy_coef = 0.1\n",
    "final_entropy_coef = 0.025\n",
    "\n",
    "batch_size = 256 # TODO - rename to mini_batch\n",
    "batch_epochs = 8\n",
    "batch_shuffle = True\n",
    "seperate_envs_shuffle = True\n",
    "\n",
    "iterations = 2048 # 2048 # TODO - rename to batch\n",
    "\n",
    "reward_normalize = True\n",
    "truncated_reward = -300\n",
    "\n",
    "debug_prints = False\n",
    "\n",
    "ppo_kwargs = {\n",
    "    'num_envs': num_envs,\n",
    "    'lr': lr,\n",
    "    'final_lr': final_lr,\n",
    "    'gamma': gamma,\n",
    "    'lam': lam,\n",
    "    'clip_eps': clip_eps,\n",
    "    'final_clip_eps': final_clip_eps,\n",
    "    'value_coef': value_coef,\n",
    "    'entropy_coef': entropy_coef,\n",
    "    'final_entropy_coef': final_entropy_coef,\n",
    "    'batch_size': batch_size,\n",
    "    'batch_epochs': batch_epochs,\n",
    "    'batch_shuffle': batch_shuffle,\n",
    "    'seperate_envs_shuffle': seperate_envs_shuffle,\n",
    "    'iterations': iterations,\n",
    "    'reward_normalize': reward_normalize,\n",
    "    'truncated_reward': truncated_reward,\n",
    "    'debug_prints': debug_prints,   \n",
    "}\n",
    "\n",
    "#ppo = PPOWrapper(envs_vector, network, **ppo_kwargs)\n",
    "ppo = PPODiscrete(envs_vector, network, **ppo_kwargs)\n",
    "\n",
    "# now = time.time()\n",
    "ppo.train(generations=100)\n",
    "# after = time.time()\n",
    "# print(f'Elapsed time: {after - now:.2f} s')\n",
    "\n",
    "\n",
    "### 5 generations ###\n",
    "# not specifed  - 48s\n",
    "# cpu:          - 236s\n",
    "# MPS:          - 110s\n",
    "#? So slow if device is selected, default is sooo much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization\n",
    "hp_optimizer = HPOptimizer(\n",
    "    env_kwargs=env_kwargs,\n",
    "    num_envs=num_envs,\n",
    "    network_class = PPONetworkDiscrete,\n",
    "    network_kwargs=network_kwargs,\n",
    "    ppo_class=PPODiscrete,\n",
    "    ppo_kwargs=ppo_kwargs,\n",
    ")\n",
    "\n",
    "parameters = [\n",
    "    #'batch_shuffle',\n",
    "    'reward_normalize',\n",
    "    'seperate_envs_shuffle',\n",
    "    #('gamma', [0.99, 0.995, 0.999]), \n",
    "    ('lam', [0.95, 0.975, 0.99]),\n",
    "    #('final_clip_eps', [0.002, 0.01, 0.05]),\n",
    "    ('final_lr', [1e-5, 1e-6, 1e-7]),\n",
    "    ('batch_size', [64, 128, 256, 512]),\n",
    "    ('batch_epochs', [2, 4, 8, 16]),\n",
    "    ]\n",
    "\n",
    "# evolutions = hp_optimizer.optimize_hyperparameters(\n",
    "#     parameters, generations=40, num_trials = 16,\n",
    "#     )\n",
    "\n",
    "\n",
    "# hp_optimizer.evolution_video(\n",
    "#     generations=100, video_folder = 'videos', increments=10, max_frames=max_episode_steps,\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPOgym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
