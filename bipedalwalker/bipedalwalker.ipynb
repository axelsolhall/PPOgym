{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar imports\n",
    "import sys, os, time, importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Gym imports\n",
    "import gym\n",
    "from gym.vector import SyncVectorEnv\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "# Custom imports\n",
    "sys.path.append(os.path.abspath('..')) # Add parent directory to path\n",
    "\n",
    "import ppo_network\n",
    "importlib.reload(ppo_network) # Prevents caching issues with notebooks\n",
    "from ppo_network import PPONetworkContinuous\n",
    "\n",
    "import ppo\n",
    "importlib.reload(ppo) # Prevents caching issues with notebooks\n",
    "from ppo import PPOContinuous\n",
    "\n",
    "import hp_tuner # TODO - Rename to hp_tuner\n",
    "importlib.reload(hp_tuner) # Prevents caching issues with notebooks\n",
    "from hp_tuner import HPOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BipedalWalker environment\n",
    "env_id = 'BipedalWalker-v3'\n",
    "max_episode_steps = 1024\n",
    "num_envs = 16\n",
    "\n",
    "env_kwargs = {\n",
    "    'id': env_id,\n",
    "    'max_episode_steps': max_episode_steps,\n",
    "}\n",
    "\n",
    "\n",
    "# Create vectorized environment\n",
    "envs_vector = SyncVectorEnv([lambda: gym.make(**env_kwargs)] * num_envs)\n",
    "states, infos = envs_vector.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy-Value Network\n",
    "# TODO - Move to PPO-kwargs\n",
    "input_dims = 24\n",
    "output_dims = 4\n",
    "\n",
    "shared_hidden_dims = [1024, 1024, 512]\n",
    "shared_norm = nn.LayerNorm\n",
    "shared_activation = nn.ReLU\n",
    "\n",
    "mean_hidden_dims = [512, 256, 128, 64]\n",
    "mean_norm = nn.LayerNorm\n",
    "mean_activation = nn.ReLU\n",
    "\n",
    "log_var_hidden_dims = [512, 256, 128, 64]\n",
    "log_var_norm = nn.LayerNorm\n",
    "log_var_activation = nn.ReLU\n",
    "\n",
    "value_hidden_dims = [512, 256, 128, 64]\n",
    "value_norm = nn.LayerNorm\n",
    "value_activation = nn.ReLU\n",
    "\n",
    "network_kwargs = {\n",
    "    'input_dims': input_dims,\n",
    "    'output_dims': output_dims,\n",
    "    \n",
    "    'shared_hidden_dims': shared_hidden_dims,\n",
    "    'shared_norm': shared_norm,\n",
    "    'shared_activation': shared_activation,\n",
    "    \n",
    "    'mean_hidden_dims': mean_hidden_dims,\n",
    "    'mean_norm': mean_norm,\n",
    "    'mean_activation': mean_activation,\n",
    "    \n",
    "    'log_var_hidden_dims': log_var_hidden_dims,\n",
    "    'log_var_norm': log_var_norm,\n",
    "    'log_var_activation': log_var_activation,\n",
    "    \n",
    "    'value_hidden_dims': value_hidden_dims,\n",
    "    'value_norm': value_norm,\n",
    "    'value_activation': value_activation,\n",
    "}\n",
    "\n",
    "network = PPONetworkContinuous(**network_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: per vectorized env: 0.01 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/axelsolhall/miniconda3/envs/PPOgym/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:253: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'torch.Tensor'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Test forward passes\n",
    "now = time.time()\n",
    "for _ in range(10):\n",
    "    states_tensor = torch.tensor(states, dtype=torch.float32)\n",
    "    mean, log_var, value = network(states_tensor)\n",
    "    std_dev = torch.exp(log_var / 2)\n",
    "    \n",
    "    actions_dist = torch.distributions.Normal(mean, std_dev)\n",
    "    actions = actions_dist.sample()\n",
    "    \n",
    "    states, rewards, dones, truncateds, infos = envs_vector.step(actions)\n",
    "print(\n",
    "    f'Elapsed time: per vectorized env: {(time.time() - now)/num_envs:.2f} s'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO \n",
    "action_dims = 4\n",
    "\n",
    "lr = 3e-4\n",
    "final_lr = 5e-6\n",
    "\n",
    "gamma = 0.99\n",
    "lam = 0.95\n",
    "\n",
    "clip_eps = 0.25\n",
    "final_clip_eps = 0.025\n",
    "\n",
    "value_coef = 0.7\n",
    "\n",
    "entropy_coef = 0.05\n",
    "final_entropy_coef = 0.025\n",
    "\n",
    "batch_size = 512 # TODO - rename to mini_batch\n",
    "batch_epochs = 8\n",
    "batch_shuffle = True\n",
    "seperate_envs_shuffle = True\n",
    "\n",
    "iterations = 2048  # TODO - rename to batch\n",
    "\n",
    "reward_normalize = True\n",
    "truncated_reward = 50\n",
    "\n",
    "debug_prints = False\n",
    "\n",
    "ppo_kwargs = {\n",
    "    'action_dims': action_dims,\n",
    "    'num_envs': num_envs,\n",
    "    'lr': lr,\n",
    "    'final_lr': final_lr,\n",
    "    'gamma': gamma,\n",
    "    'lam': lam,\n",
    "    'clip_eps': clip_eps,\n",
    "    'final_clip_eps': final_clip_eps,\n",
    "    'value_coef': value_coef,\n",
    "    'entropy_coef': entropy_coef,\n",
    "    'final_entropy_coef': final_entropy_coef,\n",
    "    'batch_size': batch_size,\n",
    "    'batch_epochs': batch_epochs,\n",
    "    'batch_shuffle': batch_shuffle,\n",
    "    'seperate_envs_shuffle': seperate_envs_shuffle,\n",
    "    'iterations': iterations,\n",
    "    'reward_normalize': reward_normalize,\n",
    "    'truncated_reward': truncated_reward,\n",
    "    'debug_prints': debug_prints,   \n",
    "}\n",
    "\n",
    "ppo = PPOContinuous(envs_vector, network, **ppo_kwargs)\n",
    "\n",
    "#ppo.train(generations=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evolution with save generations: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
      "Generation    0 - Reward:   -30.98, w/o trunc.:   -80.98\n",
      "Generation    1 - Reward:   -37.33, w/o trunc.:   -87.33\n",
      "Generation    2 - Reward:   -36.50, w/o trunc.:   -86.50\n",
      "Generation    3 - Reward:   -38.95, w/o trunc.:   -88.95\n",
      "Generation    4 - Reward:   -37.70, w/o trunc.:   -87.70\n",
      "Generation    5 - Reward:   -37.84, w/o trunc.:   -87.84\n",
      "Generation    6 - Reward:   -37.67, w/o trunc.:   -87.67\n",
      "Generation    7 - Reward:   -37.68, w/o trunc.:   -87.68\n",
      "Generation    8 - Reward:   -37.69, w/o trunc.:   -87.69\n",
      "Generation    9 - Reward:   -37.57, w/o trunc.:   -87.57\n",
      "Generation   10 - Reward:   -37.63, w/o trunc.:   -87.63\n",
      "Generation   11 - Reward:   -37.64, w/o trunc.:   -87.64\n",
      "Generation   12 - Reward:   -37.84, w/o trunc.:   -87.84\n",
      "Generation   13 - Reward:   -37.85, w/o trunc.:   -87.85\n",
      "Generation   14 - Reward:   -37.75, w/o trunc.:   -87.75\n",
      "Generation   15 - Reward:   -37.78, w/o trunc.:   -87.78\n",
      "Generation   16 - Reward:   -37.66, w/o trunc.:   -87.66\n",
      "Generation   17 - Reward:   -37.59, w/o trunc.:   -87.59\n",
      "Generation   18 - Reward:   -37.59, w/o trunc.:   -87.59\n",
      "Generation   19 - Reward:   -37.68, w/o trunc.:   -87.68\n",
      "Generation   20 - Reward:   -37.81, w/o trunc.:   -87.81\n",
      "Generation   21 - Reward:   -37.91, w/o trunc.:   -87.91\n",
      "Generation   22 - Reward:   -37.87, w/o trunc.:   -87.87\n",
      "Generation   23 - Reward:   -37.80, w/o trunc.:   -87.80\n",
      "Generation   24 - Reward:   -37.68, w/o trunc.:   -87.68\n",
      "Generation   25 - Reward:   -37.76, w/o trunc.:   -87.76\n",
      "Generation   26 - Reward:   -37.68, w/o trunc.:   -87.68\n",
      "Generation   27 - Reward:   -37.73, w/o trunc.:   -87.73\n",
      "Generation   28 - Reward:   -37.78, w/o trunc.:   -87.78\n",
      "Generation   29 - Reward:   -37.95, w/o trunc.:   -87.95\n",
      "Generation   30 - Reward:   -37.89, w/o trunc.:   -87.89\n",
      "Generation   31 - Reward:   -37.80, w/o trunc.:   -87.80\n",
      "Generation   32 - Reward:   -37.72, w/o trunc.:   -87.72\n",
      "Generation   33 - Reward:   -37.75, w/o trunc.:   -87.75\n",
      "Generation   34 - Reward:   -37.72, w/o trunc.:   -87.72\n",
      "Generation   35 - Reward:   -37.91, w/o trunc.:   -87.91\n",
      "Generation   36 - Reward:   -37.82, w/o trunc.:   -87.82\n",
      "Generation   37 - Reward:   -37.99, w/o trunc.:   -87.99\n",
      "Generation   38 - Reward:   -38.05, w/o trunc.:   -88.05\n",
      "Generation   39 - Reward:   -37.99, w/o trunc.:   -87.99\n",
      "Generation   40 - Reward:   -37.93, w/o trunc.:   -87.93\n",
      "Generation   41 - Reward:   -37.75, w/o trunc.:   -87.75\n",
      "Generation   42 - Reward:   -37.68, w/o trunc.:   -87.68\n",
      "Generation   43 - Reward:   -37.86, w/o trunc.:   -87.86\n",
      "Generation   44 - Reward:   -37.90, w/o trunc.:   -87.90\n",
      "Generation   45 - Reward:   -37.97, w/o trunc.:   -87.97\n",
      "Generation   46 - Reward:   -38.11, w/o trunc.:   -88.11\n",
      "Generation   47 - Reward:   -37.95, w/o trunc.:   -87.95\n",
      "Generation   48 - Reward:   -37.94, w/o trunc.:   -87.94\n",
      "Generation   49 - Reward:   -37.43, w/o trunc.:   -87.43\n",
      "Generation   50 - Reward:   -37.83, w/o trunc.:   -87.83\n",
      "Generation   51 - Reward:   -37.70, w/o trunc.:   -87.70\n",
      "Generation   52 - Reward:   -37.67, w/o trunc.:   -87.67\n",
      "Generation   53 - Reward:   -37.84, w/o trunc.:   -87.84\n",
      "Generation   54 - Reward:   -37.93, w/o trunc.:   -87.93\n",
      "Generation   55 - Reward:   -37.79, w/o trunc.:   -87.79\n",
      "Generation   56 - Reward:   -37.97, w/o trunc.:   -87.97\n",
      "Generation   57 - Reward:   -37.58, w/o trunc.:   -87.58\n",
      "Generation   58 - Reward:   -37.64, w/o trunc.:   -87.64\n",
      "Generation   59 - Reward:   -37.91, w/o trunc.:   -87.91\n",
      "Generation   60 - Reward:   -38.15, w/o trunc.:   -88.15\n",
      "Generation   61 - Reward:   -38.05, w/o trunc.:   -88.05\n",
      "Generation   62 - Reward:   -38.34, w/o trunc.:   -88.34\n",
      "Generation   63 - Reward:   -37.87, w/o trunc.:   -87.87\n",
      "Generation   64 - Reward:   -37.90, w/o trunc.:   -87.90\n",
      "Generation   65 - Reward:   -37.79, w/o trunc.:   -87.79\n",
      "Generation   66 - Reward:   -37.77, w/o trunc.:   -87.77\n",
      "Generation   67 - Reward:   -37.58, w/o trunc.:   -87.58\n",
      "Generation   68 - Reward:   -37.74, w/o trunc.:   -87.74\n",
      "Generation   69 - Reward:   -38.11, w/o trunc.:   -88.11\n",
      "Generation   70 - Reward:   -38.25, w/o trunc.:   -88.25\n",
      "Generation   71 - Reward:   -38.06, w/o trunc.:   -88.06\n",
      "Generation   72 - Reward:   -38.12, w/o trunc.:   -88.12\n",
      "Generation   73 - Reward:   -37.69, w/o trunc.:   -87.69\n",
      "Generation   74 - Reward:   -37.65, w/o trunc.:   -87.65\n",
      "Generation   75 - Reward:   -37.77, w/o trunc.:   -87.77\n",
      "Generation   76 - Reward:   -37.97, w/o trunc.:   -87.97\n",
      "Generation   77 - Reward:   -38.00, w/o trunc.:   -88.00\n",
      "Generation   78 - Reward:   -38.14, w/o trunc.:   -88.14\n",
      "Generation   79 - Reward:   -38.27, w/o trunc.:   -88.27\n",
      "Generation   80 - Reward:   -37.83, w/o trunc.:   -87.83\n",
      "Generation   81 - Reward:   -37.94, w/o trunc.:   -87.94\n",
      "Generation   82 - Reward:   -37.73, w/o trunc.:   -87.73\n",
      "Generation   83 - Reward:   -37.87, w/o trunc.:   -87.87\n",
      "Generation   84 - Reward:   -37.97, w/o trunc.:   -87.97\n",
      "Generation   85 - Reward:   -38.21, w/o trunc.:   -88.21\n",
      "Generation   86 - Reward:   -38.21, w/o trunc.:   -88.21\n",
      "Generation   87 - Reward:   -38.29, w/o trunc.:   -88.29\n",
      "Generation   88 - Reward:   -37.86, w/o trunc.:   -87.86\n",
      "Generation   89 - Reward:   -37.79, w/o trunc.:   -87.79\n",
      "Generation   90 - Reward:   -37.42, w/o trunc.:   -87.42\n",
      "Generation   91 - Reward:   -37.48, w/o trunc.:   -87.48\n",
      "Generation   92 - Reward:   -37.89, w/o trunc.:   -87.89\n",
      "Generation   93 - Reward:   -38.17, w/o trunc.:   -88.17\n",
      "Generation   94 - Reward:   -38.29, w/o trunc.:   -88.29\n",
      "Generation   95 - Reward:   -38.22, w/o trunc.:   -88.22\n",
      "Generation   96 - Reward:   -37.80, w/o trunc.:   -87.80\n",
      "Generation   97 - Reward:   -37.81, w/o trunc.:   -87.81\n",
      "Generation   98 - Reward:   -38.10, w/o trunc.:   -88.10\n",
      "Generation   99 - Reward:   -37.52, w/o trunc.:   -87.52\n",
      "Generation  100 - Reward:   -38.03, w/o trunc.:   -88.03\n",
      "Video saved to videos/evo_video_100_gens_5b506ae7.mp4\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter optimization\n",
    "hp_optimizer = HPOptimizer(\n",
    "    env_kwargs=env_kwargs,\n",
    "    num_envs=num_envs,\n",
    "    network_class = PPONetworkContinuous,\n",
    "    network_kwargs=network_kwargs,\n",
    "    ppo_class=PPOContinuous,\n",
    "    ppo_kwargs=ppo_kwargs,\n",
    ")\n",
    "\n",
    "parameters = [\n",
    "    ('entropy_coef', [0.1, -0.1]),\n",
    "    ('batch_size', [64, 128, 256, 512]),\n",
    "    ('batch_epochs', [2, 4, 8, 16]),\n",
    "    ]\n",
    "\n",
    "# evolutions = hp_optimizer.optimize_hyperparameters(\n",
    "#     parameters, generations=50, num_trials = 16,\n",
    "#     )\n",
    "\n",
    "\n",
    "hp_optimizer.evolution_video(\n",
    "    generations=100, video_folder = 'videos', increments=10, max_frames=max_episode_steps,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPOgym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
